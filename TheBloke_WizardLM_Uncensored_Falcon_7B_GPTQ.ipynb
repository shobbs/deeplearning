{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shobbs/deeplearning/blob/master/TheBloke_WizardLM_Uncensored_Falcon_7B_GPTQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies and Import Modules\n",
        "\n",
        "We must build AutoGPTQ from source."
      ],
      "metadata": {
        "id": "o7B20_yp8XQU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnZqZFnPsTCv"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate einops sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PanQiWei/AutoGPTQ\n",
        "!pip install ./AutoGPTQ/"
      ],
      "metadata": {
        "id": "2uAfpcVWDrNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM"
      ],
      "metadata": {
        "id": "giYyj0jtsWgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Model and Tokenizer\n",
        "\n",
        "This may take a while. It also requires around 3.5GB of RAM and 7.5GB of GPU. It will use 28 GB of disk space."
      ],
      "metadata": {
        "id": "SmFeUAB48hxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ\""
      ],
      "metadata": {
        "id": "rUCYBuA77SqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)"
      ],
      "metadata": {
        "id": "FhYXfo6X3Rdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoGPTQForCausalLM.from_quantized(model_path, device=\"cuda:0\", use_triton=False, use_safetensors=True, torch_dtype=torch.float32, trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJP5IpTO0A5A",
        "outputId": "d3b92c69-8983-4485-83fa-0ff2780bcfc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "WARNING:accelerate.utils.modeling:The safetensors archive passed at /root/.cache/huggingface/hub/models--TheBloke--WizardLM-Uncensored-Falcon-7B-GPTQ/snapshots/83fd597a3332323e06fe883f680829a498d9fa9f/gptq_model-4bit-64g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
            "WARNING:auto_gptq.modeling._base:can't get model's sequence length from model config, will set to 4096.\n",
            "WARNING:auto_gptq.modeling._base:RWGPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\n",
            "WARNING:auto_gptq.modeling._base:RWGPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Prompt\n",
        "\n",
        "You only need to change the prompt below."
      ],
      "metadata": {
        "id": "0LmPrfyW8wdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Describe a painting of a falcon hunting a llama in a very detailed way.\" # Change this to your prompt\n",
        "prompt_template = f\"### Instruction: {prompt}\\n### Response:\""
      ],
      "metadata": {
        "id": "ZCe5fAtX7iff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will generate the output:"
      ],
      "metadata": {
        "id": "rANxDNHy860j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(prompt_template, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
        "output = model.generate(input_ids=tokens, max_new_tokens=256, do_sample=True, temperature=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q92hidZk29wN",
        "outputId": "708fa16d-89b3-4659-f67f-25c5a6089f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the generated text\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cXDzm1-4qkL",
        "outputId": "3f809b09-caba-45f7-e767-9650aaeb757e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction: Describe a painting of a falcon hunting a llama in a very detailed way.\n",
            "### Response:The painting depicts a falcon perched on a rocky outcropping, with a mountainous landscape stretching out behind it. The falcon is perched on a dead llama carcass, its beady black eyes scanning the horizon for prey. The llama's fur is a dull brown color, and its eyes are half-closed in death.\n",
            "The falcon's feathers are ruffled in the wind, and its wings are spread wide, ready for flight. Its beak is sharp and curved, and its talons are poised to strike. The falcon's head is turned towards the llama, its gaze fierce and determined.\n",
            "The falcon's body is sleek and muscular, and its wingspan is immense. A long, curved talon is extended from one of its wings, ready to grab hold of its prey. The falcon's feathers are a mix of shades of brown and black, blending seamlessly with the rocky outcropping behind it.\n",
            "In the distance, the peaks of the mountains loom large, their snow-capped tops glistening in the sunlight. The sun beats down on the landscape, casting long shadows across the ground. The atmosphere of the painting is one of danger and excitement, as if the falcon and the llama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pt-_6trS9BEb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}